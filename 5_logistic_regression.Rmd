# Logistic Regression

```{r, echo=FALSE, warning=FALSE, message=FALSE}
library(caret)
library(ggplot2)
library(car)
library(verification)
```

Load the train data set on which ROSE was applied
```{r}
train_rose <- read.csv("data/train_rose.csv")
test <- read.csv("data/test.csv")
```

Remove the column MOSTYPE.41 created during one-hot encoding because it is redundant.
```{r}
train_rose  <- subset(train_rose, select = -c(MOSTYPE.41))
test <- subset(test, select = -c(MOSTYPE.41))
```

## Complete model
Fit a logistic regression model using all the variables
```{r}
complete_model <- glm(
    CARAVAN ~ .,
    data = train_rose,
    family = "binomial"
    )

summary(complete_model)
```

Confusion matrix on the test data set for the complete model
```{r}
pred <- predict(complete_model, newdata = test, type = "response")
```
ROC curve for the complete model
```{r}
roc.plot(test$CARAVAN, pred)
```
Confusion matrix for the complete model
```{r}
pred <- ifelse(pred > 0.5, 1, 0)
confusion_matrix <-  confusionMatrix(as.factor(test$CARAVAN), as.factor(pred), positive = "1")
print(confusion_matrix)
```

The model uses all the variables, but some of them may not be significant.
We can use the Variance Inflation Factor (VIF) to check for multicollinearity.

## Variance Inflation Factor
Calculate the VIF for the complete, reduced models, and the stepwise model.
Plot the results using a color map

```{r}
vif_visualizer <- function(model) {
  # Calculate the VIF for the complete model
  vif_values <- vif(model)
  # Create a data frame for plotting
  vif_data <- data.frame(variable = names(vif_values), vif = vif_values)
  # Plot VIF
  ggplot(vif_data, aes(x = reorder(variable, -vif), y = vif)) +
    geom_bar(stat = "identity", fill = "skyblue") +
    coord_flip() +
    labs(title = "Variance Inflation Factor (VIF)",
         x = "Variable",
         y = "VIF") +
    theme_minimal()
}

vif_visualizer(complete_model)
```
The VIF plot shows that are variables that cause multicollinearity. We have to
remove some varialbes to reduce the multicollinearity and to make the model model more interpretable.


## Reduced model
We can start by dropping the variables that are likely to be not different from zero and with a very small effect on the response variable. Select the variables that have a p-value less than 0.10 and a coefficient greater than 0.01. This seems to be a good compromise between the number of variables and the model s performance.

```{r}
vars <- names(which(summary(complete_model)$coefficients[,4] < 0.10 & summary(complete_model)$coefficients[,1] > abs(0.01)))
print(vars)
```

Fit a logistic regression model using only the selected variables
```{r}
vars <- c(vars, "CARAVAN")
reduced_model <- glm(
    CARAVAN ~ .,
    data = train_rose[, vars],
    family = "binomial"
    )
```

```{r}
summary(reduced_model)
```

Confusion matrix on the test data set for the reduced model
```{r}
pred <- predict(reduced_model, newdata = test, type = "response")
pred <- ifelse(pred > 0.5, 1, 0)
confusion_matrix <- confusionMatrix(as.factor(test$CARAVAN), as.factor(pred), positive = "1")
print(confusion_matrix)
```

We removed 61 variables from the complete model, and the model performance metrics did not change significantly.


## Stepwise regression
Considering the variables of the reduced model, perform a stepwise regression in order to select the best subset of variables. We use the Bayesian Information Criterion (BIC) to select the best model because we have a large number of variables and the BIC penalizes the number of variables in the model more than Akaike Information Criterion.
```{r}
# Stepwise regression
stepwise_model <- step(
    glm(
        CARAVAN ~ .,
        data = train_rose[, vars],
        family = "binomial",
    ),
    k = log(nrow(train_rose)),
    direction = "both"
)
```

```{r}
summary(stepwise_model)
```

Confusion matrix on the test data set for the stepwise model
```{r}
pred <- predict(stepwise_model, newdata = test, type = "response")
```

ROC curve for the stepwise model
```{r}
roc.plot(test$CARAVAN, pred)
```
```{r}
pred <- ifelse(pred > 0.5, 1, 0)
confusion_matrix <- confusionMatrix(as.factor(test$CARAVAN), as.factor(pred), positive = "1")
print(confusion_matrix)
```

The stepwise model has 28 variables, and the model performance metrics are similar to the reduced model.


## Bayesian Information Criterion Summary
Calculate the BIC for the complete, reduced models, and the stepwise model
```{r}
BIC(complete_model, reduced_model, stepwise_model)
```

## Interpretation and Conclusion
We reached the goal to reduce the number of variables without worsening the model performance.

The stepwise model, with 28 variables, reaches the best balanced accuracy on the test set, 0.5430. The factors that result to influence more the target variable are:

    -MOSTYPE.12 Affluent young families

    -PPLEZIER Contribution boat policies

    -PFIETS Contribution bicycle policies

    -MOSTYPE.36 Couples with teens, Married with children

    -MOSTYPE.38 Traditional families

In particular these vairables have a positive effect on the target variable, meaning that the higher the value of the variable, the higher the probability of the target variable to be 1.
The interpretation of the coefficient is note very straightforward because of the encoding and the normalization process applied to the data.

