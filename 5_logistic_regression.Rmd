# Logistic Regression

Load the train data set on which ROSE was applied
```{r}
train_rose <- read.csv("data/train_rose.csv")
test <- read.csv("data/test.csv")
```

Fit the model
```{r}
logistic_model <- glm(CARAVAN ~ ., data = train_rose, family = "binomial")
```
```{r}
summary(logistic_model)
```

The variable MOSTYPE.41 is redundant. Remove it and refit the model. 
```{r}
train_rose <- subset(train_rose, select = -MOSTYPE.41)
test <- subset(test, select = -MOSTYPE.41)
logistic_model <- glm(CARAVAN ~ ., data = train_rose, family = "binomial")
```

Make predictions of the test set
```{r}
x_test <- test[, !names(test) == "CARAVAN"]
# Make vote predictions
y_pred <- as.numeric(predict(logistic_model, x_test, type = "response") > 0.5)
```

Confusion matrix
```{r}
library(caret)
# Change the positive class to 1
confusionMatrix(factor(y_pred), factor(test$CARAVAN), positive = "1")
```

## VIF analysis
```{r}
library(car)
# Evaluate VIF for the model
vif_analysis <- vif(logistic_model)
# Get the variables whose VIF is greater than 10
variables_to_remove <- names(vif_analysis[vif_analysis > 10])
print(variables_to_remove)
```

Remove such variables
```{r}
train_rose <- train_rose[, !names(train_rose) %in% variables_to_remove]
test <- test[, !names(test) %in% variables_to_remove]
logistic_model <- glm(CARAVAN ~ ., data = train_rose, family = "binomial")
x_test <- test[, !names(test) == "CARAVAN"]
y_pred <- as.numeric(predict(logistic_model, x_test, type = "response") > 0.5)
```

```{r}
summary(logistic_model)
```

Confusion matrix
```{r}
library(caret)
# Change the positive class to 1
confusionMatrix(factor(y_pred), factor(test$CARAVAN), positive = "1")
```
